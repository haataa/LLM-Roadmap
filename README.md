# Overview
This project is dedicated to curating, organizing, and sharing the latest research developments, papers, and resources in the field of Large Language Models (LLMs). My goal is to create a comprehensive repository that serves as a valuable resource for researchers, practitioners, and enthusiasts interested in the advancements and applications of LLMs.

# Contents
- [Typical LLMs](# Typical LLMs)
- [Pretrain Method](# Pretrain Method)

# Typical LLMs
## Instruct GPT
- **Paper**:[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
- **year**:2022/02
- **summary/reason to read**: must-read paper from openai
## llama
- **Paper**:[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
- **year**:2023/02
- **summary/reason to read**: most widely used opensource llms now
## llama2
- **Paper**:[https://arxiv.org/pdf/2307.09288.pdf](https://arxiv.org/pdf/2307.09288.pdf)
- **year**:2023/07
- **summary/reason to read**: most widely used opensource llms now
## Mamba
- **Paper**:[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)
- **year**:2023/12
- **summary/reason to read**: potential archetecture to replace transformers

# Pretrain Method
## system
### DeepSpeed ZeRO
- **Paper**:[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
- **year**:2019/10
- **summary/reason to read**: commonly used pretrain framework


