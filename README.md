# Overview
This project is dedicated to curating, organizing, and sharing the latest research developments, papers, and resources in the field of Large Language Models (LLMs). My goal is to create a comprehensive repository that serves as a valuable resource for researchers, practitioners, and enthusiasts interested in the advancements and applications of LLMs.

# Contents
- [Typical LLMs](#Typical-LLMs)
- [Pretrain Method](#Pretrain-Method)
- [Instruction Finetune](#Instruction-Finetune)
- [Prompt Engineer](#Prompt-Engineer)
- [Compression](#Compression)
- [Extent Length](#Extent-Length)



# Typical LLMs
## Instruct GPT
- **Paper**:[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
- **year**:2022/02
- **summary/reason to read**: must-read paper from openai
## llama
- **Paper**:[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
- **year**:2023/02
- **summary/reason to read**: most widely used opensource llms now
## llama2
- **Paper**:[https://arxiv.org/pdf/2307.09288.pdf](https://arxiv.org/pdf/2307.09288.pdf)
- **year**:2023/07
- **summary/reason to read**: most widely used opensource llms now
## Mamba
- **Paper**:[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)
- **year**:2023/12
- **summary/reason to read**: potential archetecture to replace transformers
# Surveys
# Pretrain Method
## system
### DeepSpeed ZeRO
- **Paper**:[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
- **year**:2019/10
- **summary/reason to read**: commonly used pretrain framework
# MOE
# Instruction Finetune
- **Paper**:[Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2203.02155.pdf)
- **year**:2022/03
- **summary/reason to read**: 最早讲instruct finetune的论文之一
# RLHF
# Prompt Engineer
# PEFT
# Extent Length
## Position Interpolation
- **Paper**:[EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION](https://arxiv.org/pdf/2306.15595.pdf)
- **year**:2023/06
- **summary/reason to read**: 线性外插的方法拓展长度
## Attention Based
- **Paper**:[LM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2308.16137.pdf)
- **year**:2023/08
- **summary/reason to read**:[Lambda-shaped 注意力掩码和距离限制](https://www.xiaohongshu.com/explore/64fe8cb7000000001e021ec5)
## Fine-tuning based
# Compression
## Pruning
### SLICEGPT
- **Paper**:[SLICEGPT: COMPRESS LARGE LANGUAGE MODELS BY DELETING ROWS AND COLUMNS](https://arxiv.org/pdf/2401.15024.pdf)
- **year**:2024/01
- **summary/reason to read**:[通过删除大型语言模型中的行和列来实现模型的压缩](https://www.xiaohongshu.com/explore/65ba158a0000000008020723)
## Quantization
# MultiModa
# Agent
# RAG
# Domain Usage
